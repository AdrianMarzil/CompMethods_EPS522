{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750e9378",
   "metadata": {},
   "source": [
    "# Bayesian analysis and Monte Carlo simulation\n",
    "#### Computational Methods for Geoscience - EPS 400/522\n",
    "#### Instructor: Eric Lindsey\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a05db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful imports and settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from ipywidgets import interactive\n",
    "import datetime\n",
    "import time\n",
    "import scipy.optimize\n",
    "\n",
    "# new this week - may need to install using whichever method works for your computer.\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "# local import statements specific to our problem\n",
    "# these import a python module file that should be in our folder, not a conda package\n",
    "import fault_model\n",
    "import geod_transform\n",
    "import moment_tensor\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' # better looking figures on high-resolution screens\n",
    "# automatically reload modules when running, otherwise jupyter does not notice if our functions have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize random number generator\n",
    "# why do this? by setting the same initial seed, we can guarantee the same results each time we run the notebook\n",
    "# note, you may *not* want to do this if you want to get unpredictable results!\n",
    "RANDOM_SEED = 8927\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# not used in this lab:\n",
    "#import multiprocessing as mp\n",
    "#import scipy.fft\n",
    "#import cv2\n",
    "#import netCDF4 as nc\n",
    "#import scipy.signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcfaf90",
   "metadata": {},
   "source": [
    "## Monte Carlo example - line fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3640bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some test data that follows a line\n",
    "\n",
    "# True parameter values\n",
    "a_true = 1 \n",
    "b_true = 2\n",
    "sigma = 1\n",
    "\n",
    "# Size of dataset\n",
    "size = 50\n",
    "\n",
    "# Predictor variable (\"independent variable\", or in geophysics, \"coordinates\")\n",
    "x = 5*np.random.rand(size)\n",
    "\n",
    "# Simulate outcome (dependent variable)\n",
    "y = a_true + b_true * x + rng.normal(size=size) * sigma\n",
    "\n",
    "# simulate y errors\n",
    "yerr = sigma*np.ones(size)\n",
    "\n",
    "# plot the simulated data\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.errorbar(x,y,yerr=sigma,fmt='ks',label='noisy data',capsize=4)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74987c0-f1f9-4a52-a1e1-53369dd67d2b",
   "metadata": {},
   "source": [
    "\n",
    "Since this is a 2-parameter, linear model, it's great for testing that our code is working, since we can benchmark it against the least squares solution. \n",
    "\n",
    "Benchmarking is always a good idea - don't try out your code on new data until you know for sure it works correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b725420",
   "metadata": {},
   "source": [
    "### Solution 1. Least-squares analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model here - in this case, just a line. \n",
    "def model_line(x,*params):\n",
    "    # this first line \"unpacks\" the list [params] into elements. \n",
    "    # An easier way to pass many variables to a function.\n",
    "    alpha,beta = params\n",
    "    # Compute the model, and return the y values\n",
    "    model = alpha + beta*x\n",
    "    return model\n",
    "\n",
    "# here, p0 is the 'initial guess' for this method. \n",
    "# m is the model output, and mcov is the model covariance, or uncertainties.\n",
    "m,mcov = scipy.optimize.curve_fit(model_line,x,y,p0=[0,0])\n",
    "print('Scipy curve_fit found: a,b =',m)\n",
    "print('The true values were:  [%.8f %.8f]'%(a_true,b_true))\n",
    "print('Covariances:\\n',mcov)\n",
    "\n",
    "# to get the predicted y-values, we can just call our function my_line with the \n",
    "# model parameters determined by scipy.\n",
    "\n",
    "# Note: we introduce a special python concept here called \"unpacking\" a list - when we write *m,\n",
    "# this sends each of the elements of the list 'm' as separate arguments to the function.\n",
    "lsq_model = model_line(x,*m) \n",
    "# so the above line is equivalent to:\n",
    "# y_predicted = my_line(x,m[0],m[1])\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.errorbar(x,y,yerr=sigma,fmt='ks',label='noisy data',capsize=4)\n",
    "plt.plot(x,lsq_model, '-r', label='least-squares fit')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de92f1c",
   "metadata": {},
   "source": [
    "### Solution 2. Monte carlo sampling\n",
    "\n",
    "To run a monte carlo sampler, we need to define several funtions in addition to our \"model\" (defined above).\n",
    "\n",
    "These are the log-likelihood (misfit), log-prior, and their combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"log likelihood\" function for the sampler\n",
    "def lnlike_line(params, x, y, yerr):\n",
    "    # get the predicted model values\n",
    "    ypred = model_line(x, *params)\n",
    "    # compute the (negative) misfit - sum of the residuals squared, scaled by the data uncertainties\n",
    "    misfit = -0.5*np.sum(((y-ypred)/yerr)**2)\n",
    "    return misfit\n",
    "\n",
    "# set our priors - we use this to set bounds on the parameters.\n",
    "# return value is set to negative infinity if any parameters are outside their bounds, otherwise it is zero. \n",
    "# this is because we have taken the log() of our probability distribution. So 10^0 = 1, while 10^-np.inf = 0.\n",
    "# if we wanted gaussian priors, or other types, we could also implement them here instead of bounds.\n",
    "def lnprior_line(params):\n",
    "    # define bounds here\n",
    "    minvals = np.array([-10,-10])\n",
    "    maxvals = np.array([10,10])\n",
    "    if any(params-minvals<0) or any(params-maxvals>0): # check if any bounds are exceeded\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "#finally, this function puts together all the above, to determine the actual log(probability) of a set of parameters.\n",
    "def lnprob_line(params, x, y, yerr):\n",
    "    prior = lnprior_line(params)\n",
    "    if np.isinf(prior):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return prior + lnlike_line(params, x, y, yerr) #recall if lp not -inf, its 0, so this just returns likelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed2d7f",
   "metadata": {},
   "source": [
    "### Next: run the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a78e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect our variables to feed this function:\n",
    "# 1. package our input x,y,yerr data together:\n",
    "data = (x,y,yerr)\n",
    "\n",
    "# 2. decide the number of random walks and how many steps in each one:\n",
    "nwalkers = 50\n",
    "niter = 1000\n",
    "\n",
    "# 3. set initial guesses for each random walk\n",
    "initial=[0,0]\n",
    "# use this code to set up initial points for each random walk that are slightly different from each other\n",
    "ndim = len(initial)\n",
    "p0 = [np.array(initial) + 1e-1 * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "\n",
    "# create the sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob_line, args=data)\n",
    "\n",
    "# start the timer\n",
    "tstart = time.time()\n",
    "\n",
    "# run some initial steps to discard, so the walkers get into a reasonable starting location\n",
    "print(\"Running burn-in...\")\n",
    "p0, _, _ = sampler.run_mcmc(p0, niter)\n",
    "sampler.reset()\n",
    "\n",
    "print(\"Running production...\")\n",
    "pos, prob, state = sampler.run_mcmc(p0, niter)\n",
    "\n",
    "# end the timer\n",
    "tend = time.time()\n",
    "print(\"Done! Sampling took %f seconds.\" %(tend-tstart))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a92920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some of the samples, selected at random\n",
    "samples = sampler.flatchain\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.errorbar(x,y,yerr=sigma,fmt='ks',label='noisy data',capsize=4)\n",
    "for params in samples[np.random.randint(len(samples), size=100)]: # plot 100 random samples\n",
    "    plt.plot(x, model_line(x,*params), color=\"r\", alpha=0.1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the best fitting sample\n",
    "# np.argmax finds the location of the maximum element in an array.\n",
    "# so samples[np.argmax(sampler.flatlnprobability)] finds the location in the chain with the maximum log-likelihood,\n",
    "# and then returns the model that has that likelihood\n",
    "params_max  = samples[np.argmax(sampler.flatlnprobability)]\n",
    "print('MCMC Best fit: ',params_max)\n",
    "print('LSQ Best fit: ',m) \n",
    "\n",
    "# compute the line model for this set of parameters\n",
    "best_fit_model = model_line(x,*params_max)\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.errorbar(x,y,yerr=sigma,fmt='ks',label='input data',capsize=4)\n",
    "plt.plot(x, best_fit_model,'-r',label=\"MCMC\",linewidth=4)\n",
    "plt.plot(x, lsq_model, '-b',label='Least Squares')\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a83d8e5",
   "metadata": {},
   "source": [
    "### Plot the distribution of the samples for each parameter\n",
    "Laying out all the subplots to make a plot of all the 1D and 2D marginal PDFs would be kind of painful. We use the `corner` python library that makes this quick and easy. Note that all we are passing to it is the MxN array containing the M samples in N dimensions. There is lots more in the documentation: https://corner.readthedocs.io/en/latest/api/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the PDFs of the samples\n",
    "labels = [r'$\\alpha$',r'$\\beta$'] # this special r'$ $' format allows greek lettering or other LaTeX commands in the plot.\n",
    "myFig=plt.figure(figsize=(7,7))\n",
    "\n",
    "# lots of options to make this plot nice. Try playing around with some!\n",
    "fig = corner.corner(samples,show_titles=True,bins=30,use_math_text=True,fig=myFig,labels=labels,plot_datapoints=True,\n",
    "                    quantiles=[0.16, 0.5, 0.84],truths=[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d60d1e-52bc-4b74-8a61-7405823b4e6a",
   "metadata": {},
   "source": [
    "In the above plot, notice that the blue line (the \"true\" parameters we used to generate the original data is not at the center of the probability distribution. What do you think this means?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97810ff",
   "metadata": {},
   "source": [
    "## Example 2. Earthquake!\n",
    "\n",
    "The case above was a good benchmark, but pretty boring and not that useful. Let's try fitting some GPS data from the Ridgecrest earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GPS data\n",
    "gpsdata = pd.read_csv('GPS_Ridgecrest.csv')\n",
    "\n",
    "# I will convert them all to arrays at the start.\n",
    "gpslon = np.asarray(gpsdata[\"X\"])\n",
    "gpslat = np.asarray(gpsdata[\"Y\"])\n",
    "gps_dE = np.asarray(gpsdata[\"E\"])/100 # data are in cm, but we want meters\n",
    "gps_dN = np.asarray(gpsdata[\"N\"])/100\n",
    "\n",
    "#the Ridgecrest data do not have vertical displacements measured, but our model needs them, so we set them to zero..\n",
    "gps_dU = 0.*gps_dE\n",
    "\n",
    "# create a single vector with the locations to pass to the function\n",
    "gps_locations=np.append(gpslon,gpslat)\n",
    "\n",
    "# we also create a single vector that has all the GPS data in it\n",
    "# the layout of this vector is [E,N,U] for each point. For example, with n points:\n",
    "#gps_displacements = [E1,N1,U1,E2,N2,U2,...,En,Nn,Un]\n",
    "# for this, we need to know how many GPS points there are.\n",
    "Ngps = np.size(gpslon)\n",
    "# now, create a blank vector of the right length\n",
    "gps_displacements=np.empty((3*Ngps,),dtype=float)\n",
    "# fill it up, with indices representing every third item\n",
    "gps_displacements[0::3] = gps_dE\n",
    "gps_displacements[1::3] = gps_dN\n",
    "gps_displacements[2::3] = gps_dU\n",
    "\n",
    "# gps uncertainties - use fake estimates here\n",
    "gps_err = 0.01*np.ones(np.size(gps_displacements))\n",
    "\n",
    "# create a plot of the data\n",
    "fig = plt.figure(figsize = (7,7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlim([-119, -115.5])\n",
    "ax.set_ylim([34.5, 37])\n",
    "ax.grid()\n",
    "ax.scatter(-117.599,35.770,marker='*',c='r',s=200,label='Epicenter')\n",
    "vecscale = 1.5 # divide arrows by this number - larger number means smaller arrows\n",
    "q = ax.quiver(gpslon,gpslat,gps_dE,gps_dN, scale=vecscale)\n",
    "ax.quiverkey(q,X=0.85,Y=0.85,U=0.1, label ='10 cm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an example model and plot it with the data\n",
    "\n",
    "latc = 35.73  # locations are in degrees lon/lat\n",
    "lonc = -117.56\n",
    "depthc = 5.5e3 # units of distances are in meters\n",
    "strike = -40\n",
    "dip = 67\n",
    "L = 40e3\n",
    "W = 11e3\n",
    "strike_slip = -4   # negative means right-lateral\n",
    "dip_slip = -0.5\n",
    "\n",
    "# create the model\n",
    "Fmod=fault_model.FaultModel()\n",
    "Fmod.create_planar_model_centered(latc=latc,lonc=lonc,depthc=depthc,strike=strike,dip=dip,L=L,W=W,nL=1,nW=1)\n",
    "# get the \"G\" matrix for our particular GPS site locations (lat,lon)\n",
    "G=Fmod.get_greens(gpslat,gpslon)\n",
    "# m is the slip\n",
    "m = np.array([strike_slip,dip_slip])\n",
    "# predict GPS displacements with d=G*m\n",
    "predicted_displacements = np.matmul(G,m)\n",
    "\n",
    "print('misfit:',0.5*np.sum(((gps_displacements-predicted_displacements)/gps_err)**2))\n",
    "\n",
    "pred_E = predicted_displacements[0::3]\n",
    "pred_N = predicted_displacements[1::3]\n",
    "\n",
    "fig = plt.figure(figsize = (7,7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlim([-119, -115.5])\n",
    "ax.set_ylim([34.5, 37])\n",
    "ax.grid()\n",
    "ax.scatter(-117.599,35.770,marker='*',c='r',s=200,label='Epicenter')\n",
    "vecscale = 1.5 # divide arrows by this number - larger number means smaller arrows\n",
    "q1 = ax.quiver(gpslon,gpslat,gps_dE,gps_dN, scale=vecscale)\n",
    "q2 = ax.quiver(gpslon,gpslat,pred_E,pred_N, scale=vecscale,color='red')\n",
    "ax.quiverkey(q1,X=0.85,Y=0.85,U=0.1, label ='10 cm')\n",
    "ax.quiverkey(q1,X=0.85,Y=0.93,U=0.1, label ='model',color='red')\n",
    "plt.show()\n",
    "\n",
    "mag=moment_tensor.get_magnitude(L,W,np.sqrt(strike_slip**2+dip_slip**2)) # seismic moment is proportional to area * slip\n",
    "print('predicted magnitude is %.1f'%mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b578a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our fault modeling function for the sampler to fit.\n",
    "def fault_model_for_fitting(gps_locs,*params):\n",
    "    # expand the '[params]' list into its elements\n",
    "    latc,lonc,depthc,strike,dip,L,W,ss,ds = params\n",
    "    # for the modeling, we need the locations as x and y, not a single vector\n",
    "    Ngps=int(np.size(gps_locs)/2)\n",
    "    gpslon=gps_locs[:Ngps]\n",
    "    gpslat=gps_locs[Ngps:]\n",
    "    # create the model\n",
    "    Fmod=fault_model.FaultModel()\n",
    "    Fmod.create_planar_model_centered(latc=latc,lonc=lonc,depthc=depthc*1e3,strike=strike,dip=dip,L=L*1e3,W=W*1e3,nL=1,nW=1)\n",
    "    # get the \"G\" matrix for our particular GPS site locations (lat,lon)\n",
    "    G=Fmod.get_greens(gpslat,gpslon)\n",
    "    # m is the slip\n",
    "    m = np.array([ss,ds])\n",
    "    # predict GPS displacements with d=G*m\n",
    "    predicted_displacements = np.matmul(G,m)                                  \n",
    "    # return the predicted values\n",
    "    return predicted_displacements\n",
    "\n",
    "\n",
    "# \"log likelihood\" function for the sampler\n",
    "def lnlike_fault(params, x, y, yerr):\n",
    "    # get the predicted model values\n",
    "    ypred = fault_model_for_fitting(x, *params)\n",
    "    # compute the (negative) misfit - sum of the residuals squared, scaled by the data uncertainties\n",
    "    misfit = -0.5*np.sum(((y-ypred)/yerr)**2)\n",
    "    return misfit\n",
    "\n",
    "# set our priors - we use this to set bounds on the parameters.\n",
    "# return value is set to negative infinity if any parameters are outside their bounds, otherwise it is zero. \n",
    "# this is because we have taken the log() of our probability distribution. So 10^0 = 1, while 10^-np.inf = 0.\n",
    "# if we wanted gaussian priors, or other types, we could also implement them here instead of bounds.\n",
    "def lnprior_fault(params):\n",
    "    # define bounds here\n",
    "    minvals = np.array([34.2,-118.5,0,-60,45,10,1,-10,-2])\n",
    "    maxvals = np.array([37.2,-117,20,-20,135,200,50,10,2])\n",
    "    if any(params-minvals<0) or any(params-maxvals>0): # check if any bounds are exceeded\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "#finally, this function puts together all the above, to determine the actual log(probability) of a set of parameters.\n",
    "def lnprob_fault(params, x, y, yerr):\n",
    "    prior = lnprior_fault(params)\n",
    "    if np.isinf(prior):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return prior + lnlike_fault(params, x, y, yerr) #recall if lp not -inf, its 0, so this just returns likelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fafd86-e5a6-4537-94ca-8b51af1fdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect our variables to feed this function:\n",
    "# 1. package our input x,y,yerr data together:\n",
    "data = (gps_locations,gps_displacements,gps_err)\n",
    "\n",
    "# 2. decide the number of random walks and how many steps in each one:\n",
    "nwalkers = 20\n",
    "niter = 2000\n",
    "\n",
    "# 3. set initial guesses for each random walk\n",
    "initial=[35.7306, -117.5587, 4.25, -40.94, 66.90, 33.36, 10.29, -4.13, -0.51]\n",
    "\n",
    "# set up initial points for each random walk that are slightly different from each other\n",
    "ndim = len(initial)\n",
    "p0 = [np.array(initial) + 0.005 * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "\n",
    "# create the sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob_fault, args=data)\n",
    "\n",
    "# start the timer\n",
    "tstart = time.time()\n",
    "\n",
    "# run some initial steps to discard, so the walkers get into a reasonable starting location\n",
    "print(\"Running burn-in...\")\n",
    "p0, _, _ = sampler.run_mcmc(p0, 50)\n",
    "sampler.reset()\n",
    "\n",
    "print(\"Running production...\")\n",
    "pos, prob, state = sampler.run_mcmc(p0, niter)\n",
    "\n",
    "# end the timer\n",
    "tend = time.time()\n",
    "print(\"Done! Sampling took %f seconds.\" %(tend-tstart))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137af6f-0899-492c-9974-3d53de5b3d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = sampler.flatchain\n",
    "params_max  = samples[np.argmax(sampler.flatlnprobability)]\n",
    "print('%.4f, %.4f, %.2f, %.2f, %.2f, %.2f, %.2f, %.2f, %.2f'%tuple(params_max))\n",
    "predicted_displacements = fault_model_for_fitting(gps_locations,*params_max)\n",
    "pred_E = predicted_displacements[0::3]\n",
    "pred_N = predicted_displacements[1::3]\n",
    "\n",
    "fig = plt.figure(figsize = (7,7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlim([-119, -115.5])\n",
    "ax.set_ylim([34.5, 37])\n",
    "ax.grid()\n",
    "ax.scatter(-117.599,35.770,marker='*',c='r',s=200,label='Epicenter')\n",
    "vecscale = 1.5 # divide arrows by this number - larger number means smaller arrows\n",
    "q1 = ax.quiver(gpslon,gpslat,gps_dE,gps_dN, scale=vecscale)\n",
    "q2 = ax.quiver(gpslon,gpslat,pred_E,pred_N, scale=vecscale,color='red')\n",
    "ax.quiverkey(q1,X=0.85,Y=0.85,U=0.1, label ='10 cm')\n",
    "ax.quiverkey(q1,X=0.85,Y=0.93,U=0.1, label ='model',color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19a2ba-a4c5-487c-baf2-0d4abbbda6be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = ['lat','lon','depth','strike','dip','len','wid','ss','ds']\n",
    "myFig=plt.figure(figsize=(14,14))\n",
    "fig = corner.corner(samples,show_titles=True,bins=20,use_math_text=True,fig=myFig,\n",
    "                    labels=labels,plot_datapoints=True,quantiles=[0.16, 0.5, 0.84])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1427d79-4abb-4b9b-814a-c9b8388cfae3",
   "metadata": {},
   "source": [
    "## Bonus: more advanced sampling\n",
    "\n",
    "The MCMC sampling method we are using, implemented by the `emcee` package, is called an \"Ensemble Metropolis Hastings sampler\", and is described by Goodman & Weare (2010), http://dx.doi.org/10.2140/camcos.2010.5.65. The 'ensemble' nature of this sampler means it samples along multiple random walks simultaneously, and uses the information contained in the combination of their present locations to sample more efficiently - neatly avoiding the issues related to slow sampling in highly-correlated distributions that we discussed in class.\n",
    "\n",
    "However, we know that the Metropolis Hastings approach still leads to relatively slow convergence because the steps tend to be small, at least compared to slice sampling. If we could use an ensemble slice sampler, perhaps that would be the best of both worlds? Fortunately, such a method has been recently developed by Karamanis & Beutler (2021), https://dx.doi.org/10.1007/s11222-021-10038-2, and we can use it with the `zeus` package in Python, which follows roughly the same structure of `emcee`. \n",
    "\n",
    "Try installing this package and re-implement one of the two sampling codes above using that sampler, and make a comparison. How much faster is it? Can you get a similar-looking distribution with fewer samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f659c-aee2-479b-85ae-78d1d5ec4078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeus\n",
    "\n",
    "# collect our variables to feed this function:\n",
    "# 1. package our input x,y,yerr data together:\n",
    "data = (gps_locations,gps_displacements,gps_err)\n",
    "\n",
    "# 2. decide the number of random walks and how many steps in each one:\n",
    "nwalkers = 20\n",
    "niter = 500\n",
    "\n",
    "# 3. set initial guesses for each random walk\n",
    "initial=[35.7306, -117.5587, 4.25, -40.94, 66.90, 33.36, 10.29, -4.13, -0.51]\n",
    "# use this code to set up initial points for each random walk that are slightly different from each other\n",
    "ndim = len(initial)\n",
    "p0 = [np.array(initial) + 0.005 * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "\n",
    "# create the sampler\n",
    "zsampler = zeus.EnsembleSampler(nwalkers, ndim, lnprob_fault, args=data)\n",
    "\n",
    "# start the timer\n",
    "tstart = time.time()\n",
    "\n",
    "# zeus will automatically tune the sampler\n",
    "print(\"Running sampling...\")\n",
    "zsampler.run_mcmc(p0, niter)\n",
    "zsampler.summary # Print summary diagnostics\n",
    "\n",
    "# end the timer\n",
    "tend = time.time()\n",
    "print(\"Done! Sampling took %f seconds.\" %(tend-tstart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2c1e0-cc38-4aab-83fd-5a4b04951e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zsamples = zsampler.get_chain(flat=True)\n",
    "params_max  = zsamples[np.argmax(zsampler.get_log_prob())]\n",
    "print('%.2f, %.2f, %.2f, %.2f, %.2f, %.2f, %.2f, %.2f, %.2f'%tuple(params_max))\n",
    "predicted_displacements = fault_model_for_fitting(gps_locations,*params_max)\n",
    "pred_E = predicted_displacements[0::3]\n",
    "pred_N = predicted_displacements[1::3]\n",
    "\n",
    "fig = plt.figure(figsize = (7,7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlim([-119, -115.5])\n",
    "ax.set_ylim([34.5, 37])\n",
    "ax.grid()\n",
    "ax.scatter(-117.599,35.770,marker='*',c='r',s=200,label='Epicenter')\n",
    "vecscale = 1.5 # divide arrows by this number - larger number means smaller arrows\n",
    "q1 = ax.quiver(gpslon,gpslat,gps_dE,gps_dN, scale=vecscale)\n",
    "q2 = ax.quiver(gpslon,gpslat,pred_E,pred_N, scale=vecscale,color='red')\n",
    "ax.quiverkey(q1,X=0.85,Y=0.85,U=0.1, label ='10 cm')\n",
    "ax.quiverkey(q1,X=0.85,Y=0.93,U=0.1, label ='model',color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f34439-aa4d-464f-bfc6-71036427b775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(zsampler.get_log_prob())\n",
    "plt.ylim(-200,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6db56-733b-4a3e-8bbf-4738ec734f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = ['lat','lon','depth','strike','dip','len','wid','ss','ds']\n",
    "myFig=plt.figure(figsize=(14,14))\n",
    "fig = corner.corner(zsamples,show_titles=True,bins=20,use_math_text=True,fig=myFig,labels=labels,plot_datapoints=True,quantiles=[0.16, 0.5, 0.84])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17729e7-2c48-430b-9c41-7acf52786709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
