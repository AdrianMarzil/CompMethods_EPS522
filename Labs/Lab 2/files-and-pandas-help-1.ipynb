{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "substantial-bobby",
   "metadata": {},
   "source": [
    "# File I/O and Pandas usage\n",
    "#### Computational Methods for Geoscience - EPS 400/522\n",
    "#### Instructor: Eric Lindsey\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-shift",
   "metadata": {},
   "source": [
    "## 0. Direct Input/output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-award",
   "metadata": {},
   "source": [
    "A very simple way to get input is to use the input() command. This asks the user to type in a value, and the result will be set as a variable.\n",
    "\n",
    "Note, the input will always be a string - so you may need to convert it if you want to do calculations with it.\n",
    "\n",
    "A warning - if you run a cell that has input() and then don't enter any input, this will hang your jupyter notebook kernel until you provide the input. You may get a pop-up error like 'cell not executed due to pending input' if you try to run another cell without first providing the requested input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=input('enter input:')\n",
    "print('the input was', x)\n",
    "print('the type of input was', type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-freeze",
   "metadata": {},
   "source": [
    "## 1. File I/O\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-scotland",
   "metadata": {},
   "source": [
    "#### The basics\n",
    "Basic file I/O is done using the 'with' statement to open a file and close it automatically. \n",
    "\n",
    "The basic file I/O functions are file.write(str), file.writelines([list of str]), [list of str] = file.readlines(), or simply iterate over the lines using a for loop.\n",
    "\n",
    "Note that the 'mode' for opening a file is important - it should generally be 'r' (read) or 'w' (write), though in special cases we can also use 'rw' to edit a file in place (not covered here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('simple_file.txt','w') as file:\n",
    "    file.write('Here is a line\\n')\n",
    "    file.write('Whoops, I forgot a newline here')\n",
    "    file.write('Will this appear on a new line?\\n')\n",
    "    file.write('Last line.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('simple_file.txt','r') as file:\n",
    "    for line in file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-feeling",
   "metadata": {},
   "source": [
    "There are two things to note in the above code. First, we (intentionally) forgot a newline character '\\n' in the second line, so there is no break before the third line. This is actually useful - we can use write() statements to write any number of characters at a time, and are not restricted to whole lines.\n",
    "\n",
    "Second, note how the lines appear double-spaced when we print them out. This is because there is a newline character in the 'line' string, but print() also adds an extra newline by default. To avoid this, we should use print(line.strip()). \n",
    "\n",
    "Try fixing these issues in the above code. Clearly, this type of file I/O requires a lot of manual management of the details. It's kind of a pain, but can be very useful when we need to control those details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2f82d-7262-490b-b7cd-a6a1430c09a9",
   "metadata": {},
   "source": [
    "## 2. Pandas\n",
    "\n",
    "Pandas is a  useful library for data analysis, with great features that make reading and writing data very easy (it also has other features too!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create some data - add a bit of randomness so that it changes each time\n",
    "time=np.linspace(1,10,10)\n",
    "values=time**2 + np.random.randint(0,3,size=np.size(time))\n",
    "\n",
    "# create a pandas data frame with the values\n",
    "# note the special dictionary format: {'name':array} to give the columns names.\n",
    "df = pd.DataFrame(data={'years as faculty':time, 'number of cats':values})\n",
    "print(df)\n",
    "\n",
    "# save the data to a csv file, without the index column\n",
    "df.to_csv('cats_over_time.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from a csv file\n",
    "cats=pd.read_csv('cats_over_time.csv')\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-syndrome",
   "metadata": {},
   "source": [
    "Dataframes have an index column built in that appears when we print it out, but we might want to print it to the screen without that. Or, maybe we want it to look like a nicely formatted table instead of raw text. Here are a few options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out without an index column, by first converting it to string\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 'display' method instead of print to get a nicer table\n",
    "display(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09ae61-fd64-4457-9e74-b20c12b9a24c",
   "metadata": {},
   "source": [
    "## 3. Reading in data with missing values\n",
    "\n",
    "Sometimes, a data file has missing values. These might be simply blank, or they might be filled with a \"missing data\" value, like -999 or NaN. Let's explore how to read those files in Pandas.\n",
    "\n",
    "Note, for these examples to work you'll need to download the data files (missing1.csv, etc) from Canvas first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4c1448-b461-4085-a418-0aec388f84dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ex. 1. Missing data in a CSV file\n",
    "\n",
    "Open the file 'missing1.csv' with a text editor. For example, right click on it in the file list on the left, and choose 'Open With -> Editor'. You'll notice that some of the Y-values are missing! Whatever are we to do?\n",
    "\n",
    "Fortunately, the CSV format was designed to handle exactly this case - it's clear which values are missing, and pandas has no trouble reading it in.\n",
    "\n",
    "Note that pandas fills in the missing data with \"NaN\" - this stands for \"Not a Number\" and is a common way to denote missing data. In python, we can specify NaN with 'np.nan'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data with missing values and make a quick plot.\n",
    "data1=pd.read_csv('missing1.csv')\n",
    "print(data1)\n",
    "plt.plot(data1['time'],data1['values'],'k.') # notice I set the marker to black dots with 'k.'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c53df8-6d9f-4ae4-8be8-8d07b5a54d51",
   "metadata": {},
   "source": [
    "#### Ex. 2. Missing data in a space-delimited file\n",
    "\n",
    "Open the file 'missing2.txt' with a text editor. In this file, a single space was used to separate columns.\n",
    "\n",
    "This is not great - if we try reading it in using the standard way with 'delim_whitespace=True' or sep='\\s+', we run into trouble!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=pd.read_csv('missing2.txt',delim_whitespace=True)\n",
    "print(data2)\n",
    "plt.plot(data2['time'],data2['values'],'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e6616-6bdf-4eaf-99af-e707860cdbe7",
   "metadata": {},
   "source": [
    "These two data files should have the same data, what happened? Compare how they look when we printed them out above. The second version is all mixed up, because column 3 (uncertainty) is being interpreted as column 2 whenever there are missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b1debf-a7d1-4a82-9345-8b8381a48739",
   "metadata": {
    "tags": []
   },
   "source": [
    "To fix this, we should specify that we want exactly 1 space as our separator.\n",
    "\n",
    "*Note, I know this will work becasue I created the file. But in general, it can be very dangerous to try to read in a file like this, because we don't necessarily know if it was written out using 1 space, or 1 tab, or something else... user beware!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-contractor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data2=pd.read_csv('missing2.txt',sep=' ')\n",
    "plt.plot(data2['time'],data2['values'],'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19236d-8acb-4a6d-9e1e-02b1be74cff1",
   "metadata": {},
   "source": [
    "#### Ex. 3. Missing data filled in with a 'bad data' value. \n",
    "\n",
    "Sometimes, you'll find data files that use a large or negative number like -1 or -999 to mark bad data. Clearly, reading this data in the simple way won't work - pandas will happily interpret those values as real data. We need to tell it to mask those out. Pandas read_csv accepts an extra argument 'na_values' that we can use to tell it any values (single or list, numbers or strings) that should be interpreted as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whoops, this plot looks funny - all the \"bad data (-999)\" values are included and make the real data hard to see.\n",
    "data3=pd.read_csv('missing3.csv')\n",
    "print(data3)\n",
    "plt.plot(data3['time'],data3['values'],'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is better. Adding the na_values argument masks out those data as NaN. Now we just have the real data.\n",
    "data3=pd.read_csv('missing3.csv', na_values=-999)\n",
    "print(data3)\n",
    "plt.plot(data3['time'],data3['values'],'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce0d80-f5b7-4bc5-858b-c3c6a3956ebd",
   "metadata": {},
   "source": [
    "## 4. Logical Indexing - selecting data based on a condition\n",
    "\n",
    "Often, we may want to select data from an array based on whether it meets some condition. For example, select all the data points that are less than 10. We could do that with a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with 6 random data points between 0 and 20:\n",
    "data=1*np.random.randint(0,16,6)\n",
    "print('original',data)\n",
    "\n",
    "# create a blank array\n",
    "new_data=np.array([])\n",
    "\n",
    "# loop over all the data and decide whether to add it to the new array\n",
    "for i in range(len(data)):\n",
    "    if data[i] < 10:\n",
    "        new_data = np.append(new_data,data[i])\n",
    "        \n",
    "print('new', new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113c324-1184-4b4b-93f9-0e63cd43ef30",
   "metadata": {},
   "source": [
    "This works OK, but it's cumbersome. We had to start with a blank array, and manually fill it in. This is both confusing and difficult. \n",
    "\n",
    "#### The good news\n",
    "\n",
    "Python allows us to do something much nicer: we can use \"logical indexing\", which means selecting elements from the array directly, using a logical statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logical indexing for the same dataset:\n",
    "print('original',data)\n",
    "\n",
    "new_data = data[data<10]\n",
    "\n",
    "print('new', new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9cae8-7c76-43a0-9259-e03d44c0e558",
   "metadata": {},
   "source": [
    "This works because Python is able to interpret our expression \"data<10\" as a comparison that should be applied to each element of the array. We can see this directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)\n",
    "print(data<10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1f195-147a-457e-b21b-4d5703de7d7b",
   "metadata": {},
   "source": [
    "What happens when we run data[data<10] is that python first evaluates the logical comparison for each element in the array, comes up with this boolean array that is the same size as 'data', and finally selects from the array 'data' only the elements where the value is 'True'. Pretty simple, right?\n",
    "\n",
    "#### The bad news\n",
    "\n",
    "What happens if we want two conditions on our array? For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "data<10 and data>5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bdb21-98b8-4531-8303-a3ff1ac762c1",
   "metadata": {},
   "source": [
    "Huh. We have an error. Numpy does not know how to apply the logical operator \"and\" to arrays - we need to use the \"&\" operator instead. Note that we also need to include parentheses, because this operator has a higher order of operations precedence than the logical comparisons like < or >."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee428e-b1b5-42bf-88c8-6cdebb218186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)\n",
    "print( (data<10) & (data>5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271691a7-3ee5-4214-b124-85f885238161",
   "metadata": {},
   "source": [
    "Alternatively, we can use the special numpy function \"np.logical_and\" (or np.logical_or, etc.) to get this to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)\n",
    "print(np.logical_and(data<10, data>5))\n",
    "print(data[np.logical_and(data<10, data>5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca70519-2db0-41b5-95b2-f3354e2b7452",
   "metadata": {},
   "source": [
    "#### Getting the index values instead of a logical array\n",
    "\n",
    "Often when we do a logical operation on an array, we don't really want the long list of True/False values, we want to get the list of locations in the array where the result is true. We can get this with np.where:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.where((data<10) & (data>0)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0ce90-0fa5-4dff-9f3a-531814576472",
   "metadata": {},
   "source": [
    "This now gives us the list of locations in the logical array that were true. We can also use this in logical indexing - the result is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some variables to store the results\n",
    "logicalArray = np.logical_and(data<10, data>5)\n",
    "indexArray = np.where(logicalArray)\n",
    "\n",
    "# these should be exactly the same:\n",
    "print(data[logicalArray])\n",
    "print(data[indexArray])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dacc46-3ea4-4d40-ba94-f4f7da9073c1",
   "metadata": {},
   "source": [
    "Note that the results of np.where() are in a tuple. This doesn't affect the indexing, but can be a little annoying if we just want the list. Why does numpy do this?\n",
    "\n",
    "The tuple concept makes more sense if we consider that a more commmon use of logical indexing is for 2D arrays. Consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e717de-9dab-4538-b46e-e65790c0aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D array\n",
    "data_2d = np.array([[5, 10, 15],\n",
    "                    [2,  8, 14],\n",
    "                    [1,  9, 12]])\n",
    "\n",
    "# Find where elements are less than 10 and greater than 1\n",
    "rows, cols = np.where((data_2d < 10) & (data_2d > 1))\n",
    "\n",
    "# Print the row and column indices\n",
    "print(\"Row indices:\", rows)\n",
    "print(\"Column indices:\", cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884bfce5-8f4c-4404-a742-61a3b4d60688",
   "metadata": {},
   "source": [
    "Now we can use these index lists to access all these values at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6abd4-053a-41a9-8451-7c8441e4e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2d[rows, cols] = 0\n",
    "print(data_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6bd998-54ce-4d79-9f0a-b6ca7eaf4b7b",
   "metadata": {},
   "source": [
    "### A warning about memory and copies\n",
    "\n",
    "Python tries to be helpful when we are using arrays - it wants to save us memory. So if we do x=y, Python does not actually create a new array - it creates what is called a \"view\" into the array - this is just a list of the memory locations where the original data are stored.\n",
    "\n",
    "This can be good, because if we're working with large arrays, this saves us a lot of memory usage.\n",
    "\n",
    "However... you must always be careful when defining one array based on another, and then editing the second array.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(5) # a simple numpy array\n",
    "print('original x',x)\n",
    "# y is just a 'view' of x:\n",
    "y=x\n",
    "print('original y',y)\n",
    "\n",
    "y[0]=500 # only y was changed\n",
    "print('new y',y)\n",
    "print('new x',x) #whoa, somehow x changed too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867aa6f-5f4f-4731-a198-b9865542bfa3",
   "metadata": {},
   "source": [
    "To avoid this, if we really want to copy an array, we must use the .copy() method of arrays. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(5) # a simple numpy array\n",
    "print('original x',x)\n",
    "# y is a true copy of x:\n",
    "y=x.copy()\n",
    "print('original y',y)\n",
    "\n",
    "y[0]=500 # only y was changed\n",
    "print('new y',y)\n",
    "print('new x',x) #now x is safe!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f5dc0-a973-4bea-b4c3-6cb84a6c650b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Example. Dealing with outliers - obvious to humans but not pandas!\n",
    "\n",
    "Outliers are data points that will mess up our analysis if they are included. We should always be extra careful when removing data, because values we consider \"bad\" might actually be telling us something very interesting!\n",
    "\n",
    "However, sometimes the instrument is just broken, and we want to remove the data... this is OK, as long as we write down a clear criterion for removing the data and are honest about it.\n",
    "\n",
    "Here is a simple data file (no missing values), but some of the data are really (*really*) bad. We can tell they are bad because the third column (uncertainty) is about 100x larger than it should be. We can remove these data in a handy way using logical indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of outliers in this dataset, with large values...\n",
    "data4=pd.read_csv('bad_data.csv')\n",
    "print(data4)\n",
    "plt.plot(data4['time'],data4['values'],'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5b0bc-1d15-4a22-a720-f451354de802",
   "metadata": {},
   "source": [
    "From the plot, we can see that while the good data should be between [-1, 1], the bad data is very large - to be safe, we can be sure it's >10. So let's set that as our criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we make a copy of the dataset. This is necessary because python will try to save memory, and if we just say\n",
    "# data4_masked = data4, it will make a view of the data (just another pointer to the same data in memory).\n",
    "data4_masked = data4.copy()\n",
    "\n",
    "# now, make a 'boolean array' that tells us whether the data are greater than 10.\n",
    "bad_vals = data4_masked['values']>10\n",
    "\n",
    "# now, set those bad values to be np.nan.\n",
    "data4_masked['values'][bad_vals]=np.nan\n",
    "\n",
    "print(data4_masked)\n",
    "plt.plot(data4_masked['time'],data4_masked['values'],'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb9c01-32ff-453c-8ead-26cc573436a2",
   "metadata": {},
   "source": [
    "### Another way - Pandas queries\n",
    "\n",
    "Pandas allows us to operate on datasets in a way similar to np.logical_and() with the .query() operator. This is applied directly to our dataset; here is a brief example that removes the bad rows of data entirely from the dataset above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('bad_data.csv')\n",
    "sub_d = d.query(\"(values < 10) and (uncertainty < 10)\").copy()\n",
    "print(sub_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
