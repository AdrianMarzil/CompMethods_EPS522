{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "direct-asian",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Lab 4. Data structures and arrays\n",
    "#### Computational Methods for Geoscience - EPS 400/522\n",
    "#### Instructor: Eric Lindsey\n",
    "\n",
    "Due: Sept. 21, 2023\n",
    "\n",
    "---------\n",
    "\n",
    "Adrian Marziliano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "center-trail",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some useful imports and settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import datetime\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' # better looking figures on high-resolution screens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0cc41c-af4c-4539-9352-260e9eb3dee3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Using data structures to categorize data\n",
    "\n",
    "The file 'worldwide_m4+_2022.csv' (on canvas) contains all earthquakes larger than magnitude 4 recorded by the USGS in 2022 (more than 15,000 events). Let's use a dictionary to keep track of how many events happened in each state.\n",
    "\n",
    "First, read the data into python using pandas. The column 'place' contains a short description of the location of each event, and if it occurred in the US, this description will (usually) mention a state name. We can find out if a string is contained in another string using the keyword 'in' (see the notes).\n",
    "\n",
    "Instructions: loop over the list of state names, and for each state count the number of M4+ earthquakes that occurred in that state (you may need to loop over the whole dataset for each state name). Add this number to a dictionary with the state name as the key; for example it might contain 'New Mexico': 4.\n",
    "\n",
    "Finally, print out the top 10 states by number of earthquakes in 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c6383-10f1-4221-a403-78f8cbf45f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "earthquake_df=pd.read_csv('worldwide_m4+_2022.csv')\n",
    "print(earthquake_df[['longitude', 'latitude', 'mag', 'place']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3232da3-c280-49b4-9778-f3ba466ec9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create list of states as the keys of your dictionary.\n",
    "us_states = [ \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \n",
    "             \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
    "             \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \n",
    "             \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\",\n",
    "             \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \n",
    "             \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n",
    "             \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \n",
    "             \"Wisconsin\", \"Wyoming\"]\n",
    "\n",
    "# Create an empty dictionary to store the earthquake data\n",
    "earthquake_data_dict = {\n",
    "    \"Time\": [],\n",
    "    \"Latitude\": [],\n",
    "    \"Longitude\": [],\n",
    "    \"Depth\": [],\n",
    "    \"Magnitude\": [],\n",
    "    \"Place\": []\n",
    "}\n",
    "\n",
    "# Iterate through the DataFrame and extract data for US state names\n",
    "for index, row in earthquake_df.iterrows():\n",
    "    if isinstance(row['place'], str):  # Check if 'place' is a string\n",
    "        for state in us_states:\n",
    "            if state in row['place']:\n",
    "                earthquake_data_dict[\"Time\"].append(row['time'])\n",
    "                earthquake_data_dict[\"Latitude\"].append(row['latitude'])\n",
    "                earthquake_data_dict[\"Longitude\"].append(row['longitude'])\n",
    "                earthquake_data_dict[\"Depth\"].append(row['depth'])\n",
    "                earthquake_data_dict[\"Magnitude\"].append(row['mag'])\n",
    "                earthquake_data_dict[\"Place\"].append(row['place'])\n",
    "                break  # Break the loop once a match is found to avoid duplicate entries\n",
    "\n",
    "# Convert the dictionary to a DataFrame if needed\n",
    "earthquake_data_df = pd.DataFrame(earthquake_data_dict)\n",
    "\n",
    "# Print or use the earthquake data as needed\n",
    "print(earthquake_data_df[['Time', 'Magnitude', 'Place']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-moderator",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Resampling a dataset\n",
    "\n",
    "Often times, our data have values missing, large errors, or are unevenly sampled. In this case, we need to 'resample' the data onto a regular grid. This is also known as 'gridding' the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data - slight variation in the time sampling\n",
    "time = np.linspace(0, 10, 20) +  np.random.uniform(-0.2, 0.2, 20)\n",
    "values = np.sin(time)\n",
    "\n",
    "# add some bad data\n",
    "ibad=np.random.randint(2,18,(4,))\n",
    "values[ibad] += 5+10*np.random.rand(4)\n",
    "\n",
    "# plot the data\n",
    "plt.plot(time,values,'ks',label='original')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fefebe-70f9-443d-abc5-7766de048ee0",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Assignment 1: remove outliers and resample the above data \n",
    "\n",
    "Step 1. Remove the outliers using logical indexing.\n",
    "\n",
    "Step 2. Resample the remaining data onto a regularly spaced set of points sampled every 0.1 seconds, from 0 to 10. You can choose the interpolation method you find best!\n",
    "\n",
    "Step 3. Plot the resampled data on top of the original data (without outliers), showing how the interpolation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot US earthquake data\n",
    "plt.plot(earthquake_data_df['Time'],earthquake_data_df['Magnitude'],'k.')\n",
    "plt.title('Earthquake data for US states')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13f279-3840-45e7-a07d-b6facfb2f7d4",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### STEP 1: Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e93db9-974c-4113-9a4a-0c8ab23eb0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for the 'mag' column\n",
    "mean_magnitude = earthquake_data_df['Magnitude'].mean()\n",
    "std_deviation_magnitude = earthquake_data_df['Magnitude'].std()\n",
    "\n",
    "# Define a threshold for outliers (e.g., values more than 2 standard deviations from the mean)\n",
    "threshold = 3 * std_deviation_magnitude\n",
    "\n",
    "# Create a boolean mask identifying outliers\n",
    "outliers_mask = np.abs(earthquake_data_df['Magnitude'] - mean_magnitude) > threshold\n",
    "\n",
    "# Use the mask to filter the DataFrame and remove outliers\n",
    "filtered_data = earthquake_data_df[~outliers_mask]\n",
    "\n",
    "# Now, 'filtered_earthquake_df' contains the DataFrame with outliers removed.\n",
    "\n",
    "# You can also reset the index if needed\n",
    "filtered_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print or work with the filtered DataFrame as needed\n",
    "#print(filtered_data)\n",
    "\n",
    "# Plot US earthquake data W/O OUTLIERS\n",
    "plt.plot(filtered_data['Time'],filtered_data['Magnitude'],'k.')\n",
    "plt.title('Outliers Removed')\n",
    "plt.ylim(3.9,6.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce872032",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4497c-e825-42bf-bab1-78f6758dbc42",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### STEP 2: Resample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the 'timestamp' column is of datetime type\n",
    "filtered_data['Time'] = pd.to_datetime(filtered_data['Time'])\n",
    "\n",
    "# Create a new datetime index for the resampled data\n",
    "start_time = pd.to_datetime(\"2022-01-01T02:51:57.266Z\")\n",
    "end_time = pd.to_datetime(\"2022-12-31T06:13:27.088Z\")\n",
    "resampled_index = pd.date_range(start=start_time, end=end_time, freq='1S')\n",
    "\n",
    "# Resample the data using interpolation\n",
    "resampled_data = filtered_data.set_index('Time').reindex(resampled_index)\n",
    "\n",
    "# Interpolate missing values\n",
    "resampled_data['Magnitude'] = resampled_data['Magnitude'].interpolate(method='linear')\n",
    "\n",
    "# Filter data from 0 to 10 seconds\n",
    "start_time = pd.to_datetime(\"2022-01-01T02:51:57.266Z\") + pd.Timedelta(seconds=0)\n",
    "end_time = pd.to_datetime(\"2022-12-31T06:13:27.088Z\") + pd.Timedelta(seconds=10)\n",
    "resampled_data = resampled_data[(resampled_data.index >= start_time) & (resampled_data.index <= end_time)]\n",
    "\n",
    "# If you want to reset the index and have a clean DataFrame\n",
    "#resampled_data = resampled_data.reset_index()\n",
    "\n",
    "# Now, resampled_data contains regularly spaced earthquake magnitude data\n",
    "resampled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0cfbb1-39c6-4fc6-aa2e-acb893a0d3e6",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### STEP 3: PLOT RESAMPLED DATA OVER ORIGINAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8756c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resampled_data['Magnitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c2e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot US earthquake data W/O OUTLIERS\n",
    "#plt.scatter(resampled_data['index'], resampled_data['Magnitude'])\n",
    "\n",
    "# Assuming you have 'resampled_data' DataFrame with the 'Magnitude' column\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "\n",
    "# Plot the 'Magnitude' data\n",
    "plt.plot(resampled_data['Magnitude'], color='blue', linestyle='-', marker='o', markersize=2, label='Magnitude')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Earthquake Magnitude Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "# Show a legend if you have multiple lines\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-terminology",
   "metadata": {},
   "source": [
    "### Assignment 2. Use 2D Interpolation to fill in the continents.\n",
    "\n",
    "Remember our averaged-monthly SST dataset? (Filename: 'sst.mon.ltm.1981-2010.nc') Let's use this as a (strange) example of interpolation. Try masking out the NaNs in the grid of temperatures from September, then use griddata to fill in all the values over the continents.\n",
    "\n",
    "I think this will prove a litte challenging - good luck, work with each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is some code to get you started.\n",
    "# note you will have to copy the data file into your current folder for it to work for you.\n",
    "\n",
    "filename = 'sst.mon.ltm.1981-2010.nc'\n",
    "dataset = nc.Dataset(filename)\n",
    "\n",
    "# sst is stored as a 3D array (time,lat,lon)\n",
    "# get the grid in September\n",
    "sst_sept = dataset['sst'][8,:,:]\n",
    "\n",
    "# Hint: note that this netCDF dataset comes with a 'mask' property that lets us know which values are NaN.\n",
    "# we can access them with sst_sept.mask\n",
    "\n",
    "print('whether each point is nan:\\n',sst_sept.mask)\n",
    "\n",
    "# you can use this to extract only the valid data from any given array, if it has the same size\n",
    "zvalid = sst_sept[~sst_sept.mask]\n",
    "\n",
    "# check the shapes:\n",
    "print('shape of sst_sept is', np.shape(sst_sept))\n",
    "# notice, now it became a vector instead of an array.\n",
    "print('shape of zvalid is', np.shape(zvalid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528eeb6e-6e08-4bae-8fee-a961a82d6cb6",
   "metadata": {},
   "source": [
    "#### I suggest the following procedure:\n",
    "\n",
    "**Step 1. Generate the gridded X and Y matrices**\n",
    "\n",
    "Use np.meshgrid on the dataset['lon'] and dataset['lat'] vectors.\n",
    "Make sure to verify that your output arrays have the same size as your SST data.\n",
    "\n",
    "**Step 2. Extract the valid points from each of your 3 arrays (X, Y, SST)**\n",
    "\n",
    "Check out the hint above for how to use the mask property of the netcdf dataset.\n",
    "\n",
    "**Step 3. Choose an interpolation method and do the interpolation from the scattered valid data back to the full X and Y grids**\n",
    "\n",
    "**Step 4. Mask the ocean areas to show just the continents. You should end up with something cool!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d5a14",
   "metadata": {},
   "source": [
    "##### Step 1: Generate the gridded X and Y matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the netCDF file\n",
    "#dataset = xr.open_dataset('your_sst_data_file.nc')\n",
    "\n",
    "# Extract latitude and longitude vectors\n",
    "latitude = dataset['lat']\n",
    "longitude = dataset['lon']\n",
    "\n",
    "# Extract SST data as well (assuming the variable name is 'sst')\n",
    "sst_data = dataset['sst']\n",
    "\n",
    "print(\"Latitude shape:\", latitude.shape)\n",
    "print(\"Longitude shape:\", longitude.shape)\n",
    "print(\"SST data shape:\", sst_data.shape)\n",
    "\n",
    "# Ensure the latitude and longitude arrays have the same size as SST data\n",
    "#assert latitude.shape == sst_data.shape, \"Latitude array size doesn't match SST data size\"\n",
    "#assert longitude.shape == sst_data.shape, \"Longitude array size doesn't match SST data size\"\n",
    "\n",
    "# Create the meshgrid\n",
    "lon_grid, lat_grid = np.meshgrid(longitude, latitude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec916f",
   "metadata": {},
   "source": [
    "##### Step 2:  Extract the valid points from each of your 3 arrays (X, Y, SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f61d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have X, Y, and SST arrays\n",
    "# You already have the latitude and longitude grids (lon_grid, lat_grid) from the previous code\n",
    "\n",
    "# Create a mask to identify invalid data points in SST (e.g., fill values)\n",
    "invalid_mask = np.isnan(sst_data)\n",
    "\n",
    "# Use the invalid_mask to extract valid points from X, Y, and SST\n",
    "X_valid = X[~invalid_mask]\n",
    "Y_valid = Y[~invalid_mask]\n",
    "SST_valid = SST[~invalid_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_continent_grid(lat_range=(-90, 90), lon_range=(-180, 180), grid_resolution=1.0):\n",
    "    \"\"\"\n",
    "    Create a grid of latitude and longitude points over the continents.\n",
    "    \n",
    "    Parameters:\n",
    "    - lat_range: Tuple (min_lat, max_lat), defines the latitude range for the grid.\n",
    "    - lon_range: Tuple (min_lon, max_lon), defines the longitude range for the grid.\n",
    "    - grid_resolution: Resolution for the grid in degrees.\n",
    "\n",
    "    Returns:\n",
    "    - continent_grid_lat: 2D array of latitude points over the continents.\n",
    "    - continent_grid_lon: 2D array of longitude points over the continents.\n",
    "    \"\"\"\n",
    "    latitudes = np.arange(lat_range[0], lat_range[1] + grid_resolution, grid_resolution)\n",
    "    longitudes = np.arange(lon_range[0], lon_range[1] + grid_resolution, grid_resolution)\n",
    "\n",
    "    continent_grid_lat, continent_grid_lon = np.meshgrid(latitudes, longitudes)\n",
    "\n",
    "    return continent_grid_lat, continent_grid_lon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6156f85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/coding/times.py:832: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/indexing.py:560: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  array = array.get_duck_array()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "different number of values and points",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m Y_full \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Interpolate the SST values using griddata\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m SST_interpolated \u001b[38;5;241m=\u001b[39m \u001b[43mgriddata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSST_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_full\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m SST_interpolated \u001b[38;5;241m=\u001b[39m SST_interpolated\u001b[38;5;241m.\u001b[39mreshape(X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#STEP 4\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Define a land-sea mask (1 for land, 0 for sea)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# You may need to replace this with a proper land-sea mask\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/scipy/interpolate/_ndgriddata.py:260\u001b[0m, in \u001b[0;36mgriddata\u001b[0;34m(points, values, xi, method, fill_value, rescale)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ip(xi)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 260\u001b[0m     ip \u001b[38;5;241m=\u001b[39m \u001b[43mLinearNDInterpolator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mrescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ip(xi)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcubic\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32minterpnd.pyx:277\u001b[0m, in \u001b[0;36mscipy.interpolate.interpnd.LinearNDInterpolator.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32minterpnd.pyx:78\u001b[0m, in \u001b[0;36mscipy.interpolate.interpnd.NDInterpolatorBase.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32minterpnd.pyx:192\u001b[0m, in \u001b[0;36mscipy.interpolate.interpnd._check_init_shape\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: different number of values and points"
     ]
    }
   ],
   "source": [
    "# Load the SST dataset\n",
    "#file_path = 'C:/Users/marzi/OneDrive - University of New Mexico/EPS 522/Labs/Lab 2 Files and Figures/sst.mon.ltm.1981-2010.nc'\n",
    "#ds = xr.open_dataset(file_path)\n",
    "ds = xr.open_dataset('sst.mon.ltm.1981-2010.nc')\n",
    "\n",
    "# Create grids of longitude and latitude\n",
    "lon = ds['lon']\n",
    "lat = ds['lat']\n",
    "X, Y = np.meshgrid(lon, lat)\n",
    "\n",
    "# Extract the SST values for September\n",
    "sst_september = ds['sst'].sel(time=ds['time.month'] == 9)\n",
    "\n",
    "# Find valid data points (not NaN) in SST\n",
    "valid_indices = ~np.isnan(sst_september)\n",
    "\n",
    "# Flatten X and Y arrays\n",
    "X_flat = X.flatten()\n",
    "Y_flat = Y.flatten()\n",
    "\n",
    "# Extract the valid X and Y values\n",
    "X_valid = X_flat\n",
    "Y_valid = Y_flat\n",
    "\n",
    "# Extract the corresponding SST values\n",
    "SST_valid = sst_september.values[valid_indices.values]\n",
    "\n",
    "\n",
    "#STEP 3\n",
    "# Create a grid for the full X and Y coordinates\n",
    "X_full = X.flatten()\n",
    "Y_full = Y.flatten()\n",
    "\n",
    "# Interpolate the SST values using griddata\n",
    "SST_interpolated = griddata((X_valid, Y_valid), SST_valid, (X_full, Y_full), method='linear')\n",
    "SST_interpolated = SST_interpolated.reshape(X.shape)\n",
    "\n",
    "\n",
    "#STEP 4\n",
    "# Define a land-sea mask (1 for land, 0 for sea)\n",
    "# You may need to replace this with a proper land-sea mask\n",
    "land_sea_mask = np.where(SST_interpolated > 0, 1, 0)\n",
    "\n",
    "# Apply the mask to the SST data\n",
    "SST_continents = np.where(land_sea_mask == 1, SST_interpolated, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41f2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
