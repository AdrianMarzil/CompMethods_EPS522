{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "direct-asian",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Lab 4. Data structures and arrays\n",
    "#### Computational Methods for Geoscience - EPS 400/522\n",
    "#### Instructor: Eric Lindsey\n",
    "\n",
    "Due: Sept. 21, 2023\n",
    "\n",
    "---------\n",
    "\n",
    "Adrian Marziliano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-trail",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some useful imports and settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import netCDF4 as nc\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' # better looking figures on high-resolution screens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0cc41c-af4c-4539-9352-260e9eb3dee3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Using data structures to categorize data\n",
    "\n",
    "The file 'worldwide_m4+_2022.csv' (on canvas) contains all earthquakes larger than magnitude 4 recorded by the USGS in 2022 (more than 15,000 events). Let's use a dictionary to keep track of how many events happened in each state.\n",
    "\n",
    "First, read the data into python using pandas. The column 'place' contains a short description of the location of each event, and if it occurred in the US, this description will (usually) mention a state name. We can find out if a string is contained in another string using the keyword 'in' (see the notes).\n",
    "\n",
    "Instructions: loop over the list of state names, and for each state count the number of M4+ earthquakes that occurred in that state (you may need to loop over the whole dataset for each state name). Add this number to a dictionary with the state name as the key; for example it might contain 'New Mexico': 4.\n",
    "\n",
    "Finally, print out the top 10 states by number of earthquakes in 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c6383-10f1-4221-a403-78f8cbf45f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "earthquake_df=pd.read_csv('worldwide_m4+_2022.csv')\n",
    "print(earthquake_df[['longitude', 'latitude', 'mag', 'place']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3232da3-c280-49b4-9778-f3ba466ec9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create list of states as the keys of your dictionary.\n",
    "us_states = [ \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \n",
    "             \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
    "             \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \n",
    "             \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\",\n",
    "             \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \n",
    "             \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n",
    "             \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \n",
    "             \"Wisconsin\", \"Wyoming\"]\n",
    "\n",
    "# Create an empty dictionary to store the earthquake data\n",
    "earthquake_data_dict = {\n",
    "    \"Time\": [],\n",
    "    \"Latitude\": [],\n",
    "    \"Longitude\": [],\n",
    "    \"Depth\": [],\n",
    "    \"Magnitude\": [],\n",
    "    \"Place\": []\n",
    "}\n",
    "\n",
    "# Iterate through the DataFrame and extract data for US state names\n",
    "for index, row in earthquake_df.iterrows():\n",
    "    if isinstance(row['place'], str):  # Check if 'place' is a string\n",
    "        for state in us_states:\n",
    "            if state in row['place']:\n",
    "                earthquake_data_dict[\"Time\"].append(row['time'])\n",
    "                earthquake_data_dict[\"Latitude\"].append(row['latitude'])\n",
    "                earthquake_data_dict[\"Longitude\"].append(row['longitude'])\n",
    "                earthquake_data_dict[\"Depth\"].append(row['depth'])\n",
    "                earthquake_data_dict[\"Magnitude\"].append(row['mag'])\n",
    "                earthquake_data_dict[\"Place\"].append(row['place'])\n",
    "                break  # Break the loop once a match is found to avoid duplicate entries\n",
    "\n",
    "# Convert the dictionary to a DataFrame if needed\n",
    "earthquake_data_df = pd.DataFrame(earthquake_data_dict)\n",
    "\n",
    "# Print or use the earthquake data as needed\n",
    "print(earthquake_data_df[['Time', 'Magnitude', 'Place']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-moderator",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Resampling a dataset\n",
    "\n",
    "Often times, our data have values missing, large errors, or are unevenly sampled. In this case, we need to 'resample' the data onto a regular grid. This is also known as 'gridding' the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data - slight variation in the time sampling\n",
    "time = np.linspace(0, 10, 20) +  np.random.uniform(-0.2, 0.2, 20)\n",
    "values = np.sin(time)\n",
    "\n",
    "# add some bad data\n",
    "ibad=np.random.randint(2,18,(4,))\n",
    "values[ibad] += 5+10*np.random.rand(4)\n",
    "\n",
    "# plot the data\n",
    "plt.plot(time,values,'ks',label='original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ff955-7f61-4dd1-818b-463e014fc89e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot US earthquake data\n",
    "plt.plot(earthquake_data_df['Time'],earthquake_data_df['Magnitude'],'k.') # notice I set the marker to black dots with 'k.'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fefebe-70f9-443d-abc5-7766de048ee0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Assignment 1: remove outliers and resample the above data \n",
    "\n",
    "Step 1. Remove the outliers using logical indexing.\n",
    "\n",
    "Step 2. Resample the remaining data onto a regularly spaced set of points sampled every 0.1 seconds, from 0 to 10. You can choose the interpolation method you find best!\n",
    "\n",
    "Step 3. Plot the resampled data on top of the original data (without outliers), showing how the interpolation works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13f279-3840-45e7-a07d-b6facfb2f7d4",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### STEP 1: Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e93db9-974c-4113-9a4a-0c8ab23eb0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'earthquake_df' with relevant data\n",
    "# Replace 'earthquake_df' with the actual name of your DataFrame.\n",
    "\n",
    "# Calculate mean and standard deviation for the 'mag' column\n",
    "mean_magnitude = earthquake_data_df['Magnitude'].mean()\n",
    "std_deviation_magnitude = earthquake_data_df['Magnitude'].std()\n",
    "\n",
    "# Define a threshold for outliers (e.g., values more than 2 standard deviations from the mean)\n",
    "threshold = 2 * std_deviation_magnitude\n",
    "\n",
    "# Create a boolean mask identifying outliers\n",
    "outliers_mask = np.abs(earthquake_data_df['Magnitude'] - mean_magnitude) > threshold\n",
    "\n",
    "# Use the mask to filter the DataFrame and remove outliers\n",
    "filtered_earthquake_df = earthquake_data_df[~outliers_mask]\n",
    "\n",
    "# Now, 'filtered_earthquake_df' contains the DataFrame with outliers removed.\n",
    "\n",
    "# You can also reset the index if needed\n",
    "filtered_earthquake_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print or work with the filtered DataFrame as needed\n",
    "print(filtered_earthquake_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ea32c-0349-4c0a-a9bd-b3b3583a7b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot US earthquake data W?O OUTLIERS\n",
    "plt.plot(filtered_earthquake_df['Time'],filtered_earthquake_df['Magnitude'],'k.') # notice I set the marker to black dots with 'k.'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4497c-e825-42bf-bab1-78f6758dbc42",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### STEP 2: RESAMPLING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f961745-3a08-40e0-a115-8606a9878967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you have the 'filtered_earthquake_df' DataFrame with the relevant data\n",
    "# Replace 'filtered_earthquake_df' with the actual name of your DataFrame.\n",
    "\n",
    "# Convert the 'Time' column to a datetime object if it's not already\n",
    "filtered_earthquake_df['Time'] = pd.to_datetime(filtered_earthquake_df['Time'])\n",
    "\n",
    "# Set the 'Time' column as the DataFrame's index using .loc\n",
    "filtered_earthquake_df.set_index('Time', inplace=True)\n",
    "\n",
    "# Filter out rows with datetime values within the valid range\n",
    "filtered_earthquake_df = filtered_earthquake_df[\n",
    "    (filtered_earthquake_df['Time'] >= pd.Timestamp('1677-09-21')) &\n",
    "    (filtered_earthquake_df['Time'] <= pd.Timestamp('2262-04-11'))\n",
    "]\n",
    "\n",
    "# Resample the data to a regularly spaced set of points every 0.1 seconds from 0 to 10 seconds\n",
    "resampled_df = filtered_earthquake_df.resample('100ms').mean()\n",
    "\n",
    "# If you want to limit the resampling to a specific time range (0 to 10 seconds in this case):\n",
    "start_time = pd.to_datetime('0s')\n",
    "end_time = pd.to_datetime('10s')\n",
    "resampled_df = resampled_df.loc[start_time:end_time]\n",
    "\n",
    "# If the DataFrame contains NaN values (gaps in the data), you can fill them if needed\n",
    "resampled_df = resampled_df.fillna(method='ffill')  # Forward fill NaN values\n",
    "\n",
    "\n",
    "# Reset the index to have the 'Time' as a column again (optional)\n",
    "resampled_df.reset_index(inplace=True)\n",
    "\n",
    "# Print or work with the resampled DataFrame as needed\n",
    "print(resampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ba724-86cf-446f-90e9-7406d830f77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot US earthquake data\n",
    "plt.plot(resampled_data['Time'],resampled_data['Magnitude'],'k.') # notice I set the marker to black dots with 'k.'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-terminology",
   "metadata": {},
   "source": [
    "### Assignment 2. Use 2D Interpolation to fill in the continents.\n",
    "\n",
    "Remember our averaged-monthly SST dataset? (Filename: 'sst.mon.ltm.1981-2010.nc') Let's use this as a (strange) example of interpolation. Try masking out the NaNs in the grid of temperatures from September, then use griddata to fill in all the values over the continents.\n",
    "\n",
    "I think this will prove a litte challenging - good luck, work with each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is some code to get you started.\n",
    "# note you will have to copy the data file into your current folder for it to work for you.\n",
    "\n",
    "filename = 'sst.mon.ltm.1981-2010.nc'\n",
    "dataset = nc.Dataset(filename)\n",
    "\n",
    "# sst is stored as a 3D array (time,lat,lon)\n",
    "# get the grid in September\n",
    "sst_sept=dataset['sst'][8,:,:]\n",
    "\n",
    "# Hint: note that this netCDF dataset comes with a 'mask' property that lets us know which values are NaN.\n",
    "# we can access them with sst_sept.mask\n",
    "\n",
    "print('whether each point is nan:\\n',sst_sept.mask)\n",
    "\n",
    "# you can use this to extract only the valid data from any given array, if it has the same size\n",
    "zvalid = sst_sept[~sst_sept.mask]\n",
    "\n",
    "# check the shapes:\n",
    "print('shape of sst_sept is', np.shape(sst_sept))\n",
    "# notice, now it became a vector instead of an array.\n",
    "print('shape of zvalid is', np.shape(zvalid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528eeb6e-6e08-4bae-8fee-a961a82d6cb6",
   "metadata": {},
   "source": [
    "#### I suggest the following procedure:\n",
    "\n",
    "**Step 1. Generate the gridded X and Y matrices**\n",
    "\n",
    "Use np.meshgrid on the dataset['lon'] and dataset['lat'] vectors.\n",
    "Make sure to verify that your output arrays have the same size as your SST data.\n",
    "\n",
    "**Step 2. Extract the valid points from each of your 3 arrays (X, Y, SST)**\n",
    "\n",
    "Check out the hint above for how to use the mask property of the netcdf dataset.\n",
    "\n",
    "**Step 3. Choose an interpolation method and do the interpolation from the scattered valid data back to the full X and Y grids**\n",
    "\n",
    "**Step 4. Mask the ocean areas to show just the continents. You should end up with something cool!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dee49e-817a-47b2-af0d-fb6e811650f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
