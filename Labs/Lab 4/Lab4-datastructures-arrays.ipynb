{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "direct-asian",
   "metadata": {},
   "source": [
    "# Lab 4. Data structures and arrays\n",
    "#### Computational Methods for Geoscience - EPS 400/522\n",
    "#### Instructor: Eric Lindsey\n",
    "\n",
    "Due: Sept. 21, 2023\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful imports and settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import netCDF4 as nc\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' # better looking figures on high-resolution screens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0cc41c-af4c-4539-9352-260e9eb3dee3",
   "metadata": {},
   "source": [
    "### Using data structures to categorize data\n",
    "\n",
    "The file 'worldwide_m4+_2022.csv' (on canvas) contains all earthquakes larger than magnitude 4 recorded by the USGS in 2022 (more than 15,000 events). Let's use a dictionary to keep track of how many events happened in each state.\n",
    "\n",
    "First, read the data into python using pandas. The column 'place' contains a short description of the location of each event, and if it occurred in the US, this description will (usually) mention a state name. We can find out if a string is contained in another string using the keyword 'in' (see the notes).\n",
    "\n",
    "Instructions: loop over the list of state names, and for each state count the number of M4+ earthquakes that occurred in that state (you may need to loop over the whole dataset for each state name). Add this number to a dictionary with the state name as the key; for example it might contain 'New Mexico': 4.\n",
    "\n",
    "Finally, print out the top 10 states by number of earthquakes in 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc975dfc-4222-400a-97c7-148b597066fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this list of states as the keys of your dictionary.\n",
    "us_states = [ \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \n",
    "             \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
    "             \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \n",
    "             \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\",\n",
    "             \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \n",
    "             \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n",
    "             \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \n",
    "             \"Wisconsin\", \"Wyoming\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3232da3-c280-49b4-9778-f3ba466ec9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "buried-moderator",
   "metadata": {},
   "source": [
    "### Resampling a dataset\n",
    "\n",
    "Often times, our data have values missing, large errors, or are unevenly sampled. In this case, we need to 'resample' the data onto a regular grid. This is also known as 'gridding' the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data - slight variation in the time sampling\n",
    "time = np.linspace(0, 10, 20) +  np.random.uniform(-0.2, 0.2, 20)\n",
    "values = np.sin(time)\n",
    "# add some bad data\n",
    "ibad=np.random.randint(2,18,(4,))\n",
    "values[ibad] += 5+10*np.random.rand(4)\n",
    "# plot the data\n",
    "plt.plot(time,values,'ks',label='original')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fefebe-70f9-443d-abc5-7766de048ee0",
   "metadata": {},
   "source": [
    "### Assignment 1: remove outliers and resample the above data \n",
    "\n",
    "Step 1. Remove the outliers using logical indexing.\n",
    "\n",
    "Step 2. Resample the remaining data onto a regularly spaced set of points sampled every 0.1 seconds, from 0 to 10. You can choose the interpolation method you find best!\n",
    "\n",
    "Step 3. Plot the resampled data on top of the original data (without outliers), showing how the inerpolation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3220965-aca0-465f-bac3-ba05abec9780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "coupled-terminology",
   "metadata": {},
   "source": [
    "### Assignment 2. Use 2D Interpolation to fill in the continents.\n",
    "\n",
    "Remember our averaged-monthly SST dataset? (Filename: 'sst.mon.ltm.1981-2010.nc') Let's use this as a (strange) example of interpolation. Try masking out the NaNs in the grid of temperatures from September, then use griddata to fill in all the values over the continents.\n",
    "\n",
    "I think this will prove a litte challenging - good luck, work with each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is some code to get you started.\n",
    "# note you will have to copy the data file into your current folder for it to work for you.\n",
    "\n",
    "filename = 'sst.mon.ltm.1981-2010.nc'\n",
    "dataset = nc.Dataset(filename)\n",
    "\n",
    "# sst is stored as a 3D array (time,lat,lon)\n",
    "# get the grid in September\n",
    "sst_sept=dataset['sst'][8,:,:]\n",
    "\n",
    "# Hint: note that this netCDF dataset comes with a 'mask' property that lets us know which values are NaN.\n",
    "# we can access them with sst_sept.mask\n",
    "\n",
    "print('whether each point is nan:\\n',sst_sept.mask)\n",
    "\n",
    "# you can use this to extract only the valid data from any given array, if it has the same size\n",
    "zvalid = sst_sept[~sst_sept.mask]\n",
    "\n",
    "# check the shapes:\n",
    "print('shape of sst_sept is', np.shape(sst_sept))\n",
    "# notice, now it became a vector instead of an array.\n",
    "print('shape of zvalid is', np.shape(zvalid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528eeb6e-6e08-4bae-8fee-a961a82d6cb6",
   "metadata": {},
   "source": [
    "#### I suggest the following procedure:\n",
    "\n",
    "**Step 1. Generate the gridded X and Y matrices**\n",
    "\n",
    "Use np.meshgrid on the dataset['lon'] and dataset['lat'] vectors.\n",
    "Make sure to verify that your output arrays have the same size as your SST data.\n",
    "\n",
    "**Step 2. Extract the valid points from each of your 3 arrays (X, Y, SST)**\n",
    "\n",
    "Check out the hint above for how to use the mask property of the netcdf dataset.\n",
    "\n",
    "**Step 3. Choose an interpolation method and do the interpolation from the scattered valid data back to the full X and Y grids**\n",
    "\n",
    "**Step 4. Mask the ocean areas to show just the continents. You should end up with something cool!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dee49e-817a-47b2-af0d-fb6e811650f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
