{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares examples\n",
    "#### Computational Methods for Geoscience - EPS 400/522\n",
    "#### Instructor: Eric Lindsey\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful imports and settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "%config InlineBackend.figure_format = 'retina' # better looking figures on high-resolution screens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fitting a line to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some noisy data that loosely follows a line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some noisy data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# first, create the time steps along the x axis: every 0.1 years for 4 years\n",
    "x = np.arange(0,4,step=0.1)\n",
    "\n",
    "# generate some noise with the same size as 'x'\n",
    "noise_amplitude=2\n",
    "noise = noise_amplitude * np.random.normal(size=np.shape(x))\n",
    "\n",
    "# create a 'true' model with parameters a_true and b_true. These are the 'hidden' values we want to know\n",
    "a_true = 5 + 10*np.random.rand()\n",
    "b_true = 5 + 10*np.random.rand()\n",
    "\n",
    "# create the y-values and add some noise.\n",
    "# Question: how does the data look if you take out the noise?\n",
    "y = a_true + b_true*x + noise\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "# note, since the data have noise, we should plot them with errorbars. Here is the command to do that:\n",
    "plt.errorbar(x,y,yerr=noise_amplitude,fmt='ks',label='noisy data',capsize=4)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity: Try to fit the data by hand by guessing the values of a and b until the model looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit these values until the line looks good.\n",
    "a=0\n",
    "b=0\n",
    "\n",
    "model = a + b*x\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.errorbar(x,y,yerr=noise_amplitude,fmt='ks',label='noisy data',capsize=4)\n",
    "plt.plot(x,model,'-r',label='my model')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close did you get? Run the cell below to find out.\n",
    "\n",
    "Note, this command will print out the answer, so don't run it until you are satisfied with your estimate. Also, don't run the first cell above more than once, since the values of a and b will be different each time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"I estimated \", a, \" for a, the true value was \", a_true)\n",
    "print(\"I estimated \", b, \" for b, the true value was \", b_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting a model with linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model function $y = a + b x$, we can express this in matrix notation as follows:\n",
    "\n",
    "    [y1] = [1  x1]  *  [a]\n",
    "    [y2]   [1  x2]     [b]\n",
    "    [y3]   [1  x3]\n",
    "    [...]  [ ... ]  \n",
    "\n",
    "Reading acros, the first row of this equation becomes $y_1 = a + bx_1$. The next row has $y_2$ and $x_2$, etc. The matrix on the left is an Nx1 column of the y-values, and on the right we have an Nx2 matrix filled with first a column of ones, and then a column of the x-values, multiplied into the 2x1 matrix with the parameters we want to fit - a and b. The result of multiplying a [Nx2] with [2x1] matrix is [Nx1], so we can see that the matrix dimensions work. \n",
    "\n",
    "In various fields, you may find the matrix [Nx2] matrix labeled as $A,$ $G,$ or $J$ and described as the \"design matrix\", \"Greens functions\" or the \"Jacobian matrix\". For any linear function $d(m)$, it can be calculated by taking the partial derivatives of the function $d$ with respect to each model parameter $m$. In the case above, where our function is $y = a + b x$, we find $\\partial y/\\partial a = 1$, and $\\partial y/\\partial b = x$, which become the columns of $G$. There is no secret magic to this, this is just the way to rewrite a set of linear equations as a matrix equation. If you don't like partial derivatives, you can also think of it as \"factoring out\" the parameters from the equation to put them in their own vector.\n",
    "\n",
    "Using this method, we can express any model that is a linear function of the parameters as a matrix multiplication. In the field of geophysics this is typically written as $d=Gm$, where $G$ is our \"design matrix\" or \"greens functions\". In this case, it is filled with ones and x-values. In general, it contains all the problem-specific information, like geometry, observation points / station locations, etc. that does not relate to the particular measurement values or parameters themselves.\n",
    "\n",
    "Using the theorems of linear algebra, we can show that the solution to the matrix problem $d=Gm$ can be written as $m=(G^TG)^{-1}G^Td$. This equation is complicated to look at, but in the computer we can work it out pretty quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: create our 'design matrix'. For a linear model, this is a column of ones, next to a column of the x-coordinate of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = np.ones(np.shape(x))\n",
    "col2 = x\n",
    "# use np.column_stack to line up arrays as columns in a matrix. There is a similar function for rows, np.row_stack.\n",
    "# note the double parentheses - we have to group all the columns together as a python \"tuple\" which is like a list but with round brackets.\n",
    "G = np.column_stack((col1,col2))\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: compute the best-fitting line using least squares\n",
    "\n",
    "Recall our equation: \n",
    "\n",
    "$m=(G^TG)^{-1}G^Td$\n",
    "\n",
    "We will use the numpy commands for transpose and inverse:\n",
    "\n",
    "    np.transpose()\n",
    "    np.linalg.inv()\n",
    "    \n",
    "And, to multiply matrices, use the .dot() method of numpy arrays, like so:\n",
    "\n",
    "    A.dot(B)\n",
    "\n",
    "Or, you can use np.matmul():\n",
    "\n",
    "    np.matmul(A,B)\n",
    "\n",
    "Note that if you literally write A * B for numpy arrays, it will try to multiply them element-wise, rather than doing matrix multiplication. Watch out!\n",
    "\n",
    "Finally, remember that our \"data\" is the vector of observed values and is called \"y\" in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the equation m = (G^T * G)^-1 * G^T * d, using numpy:\n",
    "\n",
    "m = np.linalg.inv(np.transpose(G).dot(G)).dot(np.transpose(G)).dot(y)\n",
    "\n",
    "\n",
    "# print out the result, and compare to the true values from above.\n",
    "print('The least squares answer is', m)\n",
    "print('The true values were', a_true, b_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, these values should be pretty close to our true values but not exactly equal - this is because we added noise! In general, the presence of noise means we will never be able to exactly measure the **true model** values. \n",
    "\n",
    "If you like, try running it again without noise - you should find that the estimated and true values are equal. (If they are not exactly equal, this may come from floating-point errors accumulated during the matrix inversion step. In the field of Computer Science, lots of effort has gone into designing ways of reducing this floating-point error in the computer.)\n",
    "\n",
    "Now, let's plot the model on top of the data and see how it fits. Remember, we can get our \"predicted\" data (y-values of the model) with the equation $d = G * m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_predicted is just G*m, with the m values we determined above.\n",
    "data_predicted = G.dot(m)\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.errorbar(x,y,yerr=noise_amplitude,fmt='ks',label='noisy data',capsize=4)\n",
    "plt.plot(x,data_predicted, '-r', label='least-squares fit')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fitting the data with python automatically\n",
    "\n",
    "Since least-squares fitting is such a common activity, there are nice, easy functions in python to do it. Let's use one called scipy.optimize.curve_fit.\n",
    "\n",
    "For this method, we must define our linear model as a function, whose name we pass to curve_fit along with the observation vector x, the data y, and an initial estimate of the model parameters (called \"p0\" by scipy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_line(x,a,b):\n",
    "    return a + b*x\n",
    "\n",
    "# here, p0 is the 'initial guess' for this method. \n",
    "# m is the model output, and mcov is the model covariance, or uncertainties.\n",
    "m2,mcov = scipy.optimize.curve_fit(my_line,x,y,p0=[0,0])\n",
    "print('Linear algebra found:',m)\n",
    "print('Scipy curve_fit found:',m2)\n",
    "print('The true values were:  [%.8f %.8f]'%(a_true,b_true))\n",
    "\n",
    "# to get the predicted y-values, we can just call our function my_line with the \n",
    "# model parameters determined by scipy.\n",
    "\n",
    "# Note: we introduce a special python concept here called \"unpacking\" a list - when we write *m,\n",
    "# this sends each of the elements of the list 'm' as separate arguments to the function.\n",
    "y_predicted = my_line(x,*m2) \n",
    "# so the above line is equivalent to:\n",
    "# y_predicted = my_line(x,m[0],m[1])\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.errorbar(x,y,yerr=noise_amplitude,fmt='ks',label='noisy data',capsize=4)\n",
    "plt.plot(x,y_predicted, '-r', label='least-squares fit')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Fit some data that is not linear! \n",
    "\n",
    "We create some synthetic data that has a slope, interecept and also has a step function, which we can model using the numpy function np.heaviside(). Can you modify the scipy function to fit all 3 parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some 'true' model parameter values\n",
    "a_true = 10*np.random.rand()\n",
    "b_true = 2*np.random.rand()\n",
    "c_true = 30*np.random.rand()\n",
    "d_true = 2*np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the error function (erf) to create a step function that is slightly smooth, so scipy can optimize it. \n",
    "# We should use the heaviside function instead, but it is not differentiable, which causes the curve_fit method to not work well...\n",
    "from scipy.special import erf\n",
    "\n",
    "def my_smooth_function(x, a, b, c, d):\n",
    "    return a + b*x + c * 0.5 * (1 + erf(200 * (x - d)))\n",
    "\n",
    "# define our independent variable - the \"x\" locations of the data points\n",
    "x=np.linspace(0,4,100)\n",
    "\n",
    "\n",
    "y_nonoise=my_smooth_function(x,a_true,b_true,c_true,d_true)\n",
    "y = y_nonoise + np.random.normal(size=np.shape(x))\n",
    "\n",
    "\n",
    "# use scipy.optimize.curve_fit here to fit the model with 3 parameters\n",
    "m3,mcov = scipy.optimize.curve_fit(my_smooth_function,x,y,p0=[0,0,0,2])\n",
    "\n",
    "# get the predicted y-values\n",
    "y_predicted = my_smooth_function(x,*m3) \n",
    "\n",
    "print('Scipy curve_fit estimated', m3)\n",
    "print('The true values were      [%.8f %.8f %.8f %.8f]'%(a_true,b_true,c_true,d_true))\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.errorbar(x,y,yerr=noise_amplitude,fmt='ks',label='noisy data',capsize=4)\n",
    "plt.plot(x,y_predicted, '-r', label='least-squares fit')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in the code above, the step function time (parameter \"d\") is not always accurately estimated! This turns out to be a generally difficult thing to do, since this is a very non-linear parameter. In my experimentation, I found that a good way to improve the results is to provide a visual estimate of the time of the offset in our \"p0\" parameter; this usually gives scipy a good start. On the flip side, a very bad guess will usually lead to bad results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
